'''
---------------- Training Procedure
For the train function we have to replace FLAGS.image_dim by 224
We define a function to setup the train_op: _setup_vgg_training_op
We also define a function to get the pretrained checkpoint: _get_init_pretrained_fn_vgg16
'''

def train_vgg():
    """Trains an Inception v3 network to regress joint co-ordinates (NUM_JOINTS
    sets of (x, y) co-ordinates) directly.
    """
    with tf.Graph().as_default():
        with tf.device('/cpu:0'):
            # TODO(brendan): Support multiple GPUs?
            assert FLAGS.num_gpus == 1

            training_batch = setup_train_input_pipeline(
                FLAGS.data_dir,
                FLAGS.num_readers,
                FLAGS.input_queue_memory_factor,
                FLAGS.batch_size,
                FLAGS.num_preprocess_threads,
                224) # Note because of VGG

            global_step, optimizer = _setup_optimizer(FLAGS.batch_size,
                                                      FLAGS.num_epochs_per_decay,
                                                      FLAGS.initial_learning_rate,
                                                      FLAGS.learning_rate_decay_factor)

            train_op = _setup_vgg_training_op(training_batch,
                                          global_step,
                                          optimizer)

            # TODO(brendan): track moving averages of trainable variables

            tf_logging._logger.setLevel(INFO)

            slim.learning.train(
                train_op=train_op,
                logdir='logs/',
                log_every_n_steps=10,
                global_step=global_step,
                init_fn=_get_init_pretrained_fn_vgg16(),
                session_config=tf.ConfigProto(allow_soft_placement=True))
#------------------------------------------ Setup train_op
# Function for setting up the vgg training op
# For this we define a function _inference_vgg and a function _get_vgg_variables_to_train()
def _setup_vgg_training_op(training_batch, global_step, optimizer):
    """Sets up inference (predictions), loss calculation, and minimization
    based on the input optimizer.

    Args:
        training_batch: Batch of preprocessed examples dequeued from the input
            pipeline.
        global_step: Training step counter.
        optimizer: Optimizer to minimize the loss function.

    Returns: Operation to run a training step.
    """
    with tf.device(device_name_or_function='/gpu:0'):
        loss = _inference_vgg(training_batch)

        train_op = slim.learning.create_train_op(
            total_loss=loss,
            optimizer=optimizer,
            global_step=global_step,
            variables_to_train=_get_vgg_variables_to_train())

    return train_op

# VGG inference function


def _inference_vgg(training_batch):
    """Sets up a VGG16 model, computes predictions on input images and
    calculates loss on those predictions based on an input sparse vector of
    joints (the ground truth vector).

    TF-slim's `arg_scope` is used to keep variables (`slim.model_variable`) in
    CPU memory. 

    Args:
        training_batch: A batch of training images with associated joint
            vectors.

    Returns:
        Tensor giving the total loss (combined loss from auxiliary and primary
        logits, added to regularization losses).
    """
    # What is slim.arg
    with slim.arg_scope([slim.model_variable], device='/cpu:0'):
        with slim.arg_scope(vgg.vgg_arg_scope()):
            logits, endpoints = vgg.vgg_16(inputs=training_batch.images,
                                                       num_classes=2*NUM_JOINTS)

            with tf.name_scope('summaries'):
                for activation in endpoints.values():
                    tensor_name = activation.op.name
                    tf.summary.histogram(name=tensor_name + '/activations',values=activation)
                    tf.summary.scalar(name=tensor_name + '/sparsity',tensor=tf.nn.zero_fraction(value=activation))

            x_dense_joints, y_dense_joints, weights = _sparse_joints_to_dense(training_batch)
            dense_joints = tf.concat(concat_dim=1,values=[x_dense_joints, y_dense_joints])

            slim.losses.mean_squared_error(predictions=logits,
                                           labels=dense_joints,
                                           weights=weights)

            # TODO(brendan): Calculate loss averages for tensorboard

            total_loss = slim.losses.get_total_loss()

    return total_loss


def _get_vgg_variables_to_train():
    """Returns the set of trainable variables, given by the `trainable_scopes`
    flag if passed, or all trainable variables otherwise.
    """
    list_of_trainable_variables = tf.trainable_variables()
    scopes2train = ['vgg_16/fc']

    variables_to_train = []
    for scope in scopes2train:
        variables = tf.get_collection(key=tf.GraphKeys.TRAINABLE_VARIABLES,
                                      scope=scope)
        variables_to_train.extend(variables)

    return variables_to_train

#-------------------------------------- Initializing pretrained weights

def _get_init_pretrained_fn_vgg16():
    """Returns a function that initializes the model in the graph of a passed
    session with the variables in the file found in `FLAGS.checkpoint_path`,
    except those excluded by `FLAGS.checkpoint_exclude_scopes`.
    """
    exclusions = ['vgg_16/fc']

    variables_to_restore = []
    for var in slim.get_model_variables():
        excluded = False
        for exclusion in exclusions:
            if var.op.name.startswith(exclusion):
                excluded = True
                break
        if not excluded:
            variables_to_restore.append(var)

    return slim.assign_from_checkpoint_fn(model_path='checkpoints/vgg_16.ckpt',
                                          var_list=variables_to_restore)
