\documentclass[a4paper, 12pt]{article}

\usepackage{cite} \usepackage{amsmath} \usepackage{fullpage} \usepackage{url}
\usepackage{graphicx} \usepackage{inputenc} \usepackage{titling}

\date{\today} \title{Problem Statement\\ McMaster Text to Motion Database \\CS
4ZP6}

\author{Brendan Duke\\ Andrew Kohnen\\ Udip Patel\\ Dave Pitkanen\\ Jordan
Viveiros}

\begin{document}

\maketitle \clearpage

\section{Project Overview}

The McMaster Text to Motion Database is a component of a larger project that is
being managed by Dr.Graham Taylor from the University of Guelph. The larger
project is a collaboration between the University of Guelph, SRI (a non-profit
research organization) and other institutions, and was established with the goal
of producing a "Computational Storytelling system". \\ \\ This large-scale
Computational Storytelling system is intended to work by taking text
descriptions of a scene or dialogue and producing an animated video 'story' with
that given content and characters. \\ \\ Under Dr. Wenbo He from McMaster
University, this smaller project will contribute a Python HTTP server that can
be used to process images and videos with a deep learning algorithm, along with
a website and database that can be used to store and view the processed media.
Having access to the processed media will be a resource that can be of use to
the larger project.

\section{Problem Statement and Our Contribution} To implement the 'Computational
Storytelling' functionality, the larger system should be able to produce an
animation with moving characters that look reasonably realistic. \\ \\ As of
now, there is no simple, accessible database that links images and videos with
text annotations or descriptions of the motions/positions of people observed in
the image or video (ex. mapping the locations of some of the observed skeletal
joints of a person in the image or video) \\ \\ We propose to harvest the motion
data (joint positions) of moving people by seeding our database with existing
large image/video datasets of people doing common movements, and categorizing
the processed media with tags. The developers of the larger system can use our
website to search for specific actions, and take the motion data in order to
generate the animated story with moving characters. The developers can also
upload their own media to be processed as well at any time.

\section{Motivation} The ability to move between natural language and motion
data is at the core of what the larger project is trying to acheive. To enable
research along the lines of converting text descriptions into observable motion,
we propose to create a large databank of images and videos that are annotated
both with human actions (via tags) and the motions observed (the positions of
joints relative to each other).

\section{Background Information and Challenges} Over the last ten years machine
learning algorithms have significantly improved as well as the limits of our
computer processing power, computer memory and available sources of many types
of data. The growth in all these areas has made it possible for computer
applications that use machine learning to improve their performance. The key
advancement in the algorithms of machine learning has been the ability to train
neural networks that are more than a single layer deep. With this ability neural
networks are able to learn structures from long sequences of sequential
data(long sequences of inputs such as natural language and video in our case). \\ \\
Many different researchers have already been able to create a mapping between
text and images (Kiros ​ et al., 2014a, b; ​ Socher et al., 2010). However these
examples simply map from image to text but the problem of performing the mapping
using videos is that there is now a new dimension of time and the volume of data
grows very quickly as the length of the video increases. \\ \\ The biggest
challenge of this project is to produce a deep learning model that is trained to
analyze an image or video (a set of image frames) and estimate the joint
positions of a human, if there is one in the media. Luckily, there are some
existing open-source code bases that work with varying platforms and frameworks
(like C/C++ or Python). Implementing these models will take up the bulk of the
project research. \\ \\ Once the website can take in large amounts of media
input from users and process the media, the databank can be populated by
existing collections of images/videos that are pre-labeled with the actions
observed in the media

The datasets with heavy labelling that we intend to use to seed the database of
our website are: MovieQA, Charades, MSR-VTT

\section{Objectives, Constraints and Deliverables}

The goal of the entire collaboration is to create a basic text-to-motion tool
which can link language concepts to a data structure representing motion (e.g. a
series of joint angles). The first step in analyzing motion from a machine
learning approach is typically to create a feature space -- which is a lower
dimensional representation of some of the important aspects of the data which
the machine learning algorithms can then analyze. \\ \\ The specific feature
space we wish to consider is called a pose structure (Wei, 2016). This structure
is used to model the motion of people. The feature space essentially consists of
a labelling of the regions in an image where a specific limb is located (arm,
head eye etc). Modelling the motion of the entire body is made much simpler when
the problem can be decomposed into the “independent” motion of these features
(applying the constraints that the features are connected in the conventional
human form). \\ \\ Therefore in order to achieve this first step we
hierarchically break this set of deliverables into 2 components

\end{document} \grid
