% Original work copyright 2014 Jean-Philippe Eisenbarth
% Modified work copyright 2016 of Brendan Duke and Jordan Viveiros.

% This program is free software: you can 
% redistribute it and/or modify it under the terms of the GNU General Public 
% License as published by the Free Software Foundation, either version 3 of the 
% License, or (at your option) any later version.
% This program is distributed in the hope that it will be useful,but WITHOUT ANY 
% WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A 
% PARTICULAR PURPOSE. See the GNU General Public License for more details.
% You should have received a copy of the GNU General Public License along with 
% this program.  If not, see <http://www.gnu.org/licenses/>.

% Based on the code of Yiannis Lazarides
% http://tex.stackexchange.com/questions/42602/software-requirements-specification-with-latex
% http://tex.stackexchange.com/users/963/yiannis-lazarides
% Also based on the template of Karl E. Wiegers
% http://www.se.rit.edu/~emad/teaching/slides/srs_template_sep14.pdf
% http://karlwiegers.com
\documentclass{scrreprt}
\usepackage{listings}
\usepackage{underscore}
\usepackage[bookmarks=true]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{xcolor}
\usepackage{indentfirst}
\usepackage[section]{placeins}
\usepackage{graphicx}
\usepackage{graphics}
\hypersetup{
    bookmarks=false,    % show bookmarks bar?
    pdftitle={Software Requirement Specification},    % title
    pdfauthor={Jean-Philippe Eisenbarth},                     % author
    pdfsubject={TeX and LaTeX},                        % subject of the document
    pdfkeywords={TeX, LaTeX, graphics, images}, % list of keywords
    colorlinks=true,       % false: boxed links; true: colored links
    linkcolor=blue,       % color of internal links
    citecolor=black,       % color of links to bibliography
    filecolor=black,        % color of file links
    urlcolor=purple,        % color of external links
    linktoc=page            % only page is linked
}%
\def\myversion{0.0 }
\date{}
%\title{%

%}
\usepackage{hyperref}
\begin{document}

\begin{flushright}
    \rule{16cm}{5pt}\vskip1cm
    \begin{bfseries}
        \Huge{SOFTWARE REQUIREMENTS\\ SPECIFICATION}\\
        \vspace{1.4cm}
        for\\
        \vspace{1.4cm}
        CS 4ZP6 Capstone Project\\
        \vspace{1.4cm}
        \LARGE{Version \myversion}\\
        \vspace{1.4cm}
        Prepared by Brendan Duke, Andrew Kohnen, Udip Patel, David Pitkanen, Jordan Viveiros\\
        \vspace{1.4cm}
        Using Volere Template Edition 13\\
        \vspace{1.4cm}
        McMaster Text to Motion Database\\
        \vspace{1.4cm}
        \today\\
    \end{bfseries}
\end{flushright}

\tableofcontents

\chapter*{Revision History}

\begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
            Name & Date & Reason For Changes & Version\\
        \hline
	    Brendan Duke & Oct. 7th, 2016 & Initial Version & 0.0\\
        \hline
    \end{tabular}
\end{center}

\newcounter{ConstraintNumber}
\newcounter{RequirementNumber}

% Requirement template -------------------------------------------------------
\newcommand{\requirement}[9]{%
\fbox{\parbox{\textwidth}{%
\parbox[t]{.333\textwidth}{\raggedright% 
\textbf{Req. \#}: \refstepcounter{RequirementNumber} \arabic{RequirementNumber} \label{#1}}%
\parbox[t]{.333\textwidth}{\centering% 
\textbf{Req. Type}: #2}%
\parbox[t]{.333\textwidth}{\raggedleft%
\textbf{Use Case \#}: \ref{#3}}
\newline\\
\textbf{Description}: #4\\\\
\textbf{Rationale}: #5\\\\
\textbf{Originator}: #6\\\\
\textbf{Fit Criterion}: #7\\\\
\textbf{Priority}: #8 \hfill \textbf{History}: #9\\\\
}}}
% End Requirement template ---------------------------------------------------

\chapter{Project Drivers}

\section{The Purpose of the Project}

\subsection{The User Business or Background of the Project Effort}

There is an existing project at the University of Guelph that aims to create a
system for ``computational storytelling''. The goal of the computational
storytelling project is to create a system that takes as input a basic story
composed of five sentences, and outputs an animated movie based on the story,
which is produced in collaboration between an AI and human director.

As an initial step in the computational storytelling project, the University of
Guelph group requires a database of ``human motion'' that is stored with rich
text annotations. Such a database is required as a source of training data for
the computational storytelling project to use in their methods to convert text
to animated motion.

No satisfactory database of human motion data that is stored with associated
text descriptions exists currently. However, there are existing databases of
videos of people doing various actions with accompanying text describing those
actions, for example the Charades database or MSR-VTT.

Our McMaster group has been approached to assist in this initial step in the
computational storytelling project in two ways.  Firstly, we are to develop
software, based on existing research, that is able to process video and derive
human motion data (e.g. joint positions over time) from the video. Secondly, we
are to utilize data from an existing database that already has text
annotations, such as Charades, using our video-to-motion processing software to
generate a new database that contains both rich text annotations and motion data.

\subsection{Goals of the Project}

The goal of this project is to create a database, web-interface to said
database, and a deployable software bundle providing access to
already-established human pose estimation methods. Creating this database,
website and software suite will allow the larger text-to-motion project to use
the relationships between motion data and text annotations developed through
the pose estimation software in order to provide a pose and word pairing, which
can be used for animation.

\section{The Client, the Customer, and Other Stakeholders}

\subsection{The Client}

The current clients for this project are Dr.\ Taylor and his graduate student
Thor Jonsson. Dr.\ Taylor is the primary driver to develop a website and
database where annotated motion information can be generated and pulled from as
a growth point into the larger text to motion project. They will be using the
database to train Recurrent Neural Networks (RNNs) that will pair actions and
their pose found within the database to words or combinations found in the
input story.

\subsection{The Customer}

The customers are included within the clients since building this database and
website combination will be utilized by Dr.\ Taylors research team and their
external partners. In addition to Dr.\ Taylor and his research team this project
would appeal to anyone that needed a pairing of actions and pose estimations as
the website would be readily available to others.

In general, customers of the product will be researchers in the machine
learning community who are interested in multi-modal learning, and specifically
in systems that link text to human motion. Said customers will have a high
degree of knowledge related to machine learning theory. However, they cannot be
assumed to have a high degree of skill in any programming language with a steep
skill curve, such as C++ or Haskell. Also, the customer is unlikely to be
willing to invest a large amount of time in learning how to use the software
produced by the McMaster Text-to-Motion project.

\subsection{Other Stakeholders}

Other stakeholders affected by the project include Dr.\ He, our group's internal
supervisor and teacher of the CS 4ZP6 Capstone Project course, and the team
members of our group.

Dr.\ He is a professor at McMaster who may not have the same specialized
research knowledge as members of Dr.\ Taylor's group. Dr.\ He requires an
explanation of all aspects of the project, as she will be responsible for
assigning a grade to the entire group. Dr.\ He will require updates on the
progress of the group in the form of deliverables that are part of the CS 4ZP6
syllabus.

The members of our CS 4ZP6 capstone group, namely Brendan Duke, Andrew Kohnen,
Udip Patel, David Pitkanen and Jordan Viveiros, are also stakeholders affected
by the project. For the most part, our group members did not have any
specialized knowledge related to the project before commencing, although that
knowledge is being acquired as the project progresses. The group members will
require full involvement in all aspects of the project, as well as supporting
knowledge and direction from Thor Jonsson and Dr.\ Taylor.

\section{Users of the Product}

\subsection{The Hands-on Users of the Product}

\begin{center}
    \begin{tabular}{ | p{2cm} | p{4cm} | p{2cm} | p{4cm} |}
    \hline
    User Category & User Role & Subject Matter Experience &
    Technological Experience \\ \hline
    Dr.\ Taylor's Group & Using the database to train an RNN to create
            animations from text. & Master & Master \\
    Other machine learning researchers & Using the product for any multi-modal
            machine learning use-case involving text and human motion. & Master
            & Journeyman. This user category cannot be assumed to have a high
            degree of skill in complex programming languages such as C++. \\
    Amateur machine learning enthusiasts & Using the McMaster Text-to-Motion
            database and software suite to learn about multi-modal machine
            learning and human pose estimation. & Journeyman & Journeyman \\
    \hline
    \end{tabular}
\end{center}

\subsection{Priorities Assigned to Users}

Our \textbf{key users} are members of Dr.\ Taylor's research group.
\textbf{Secondary users} are other members of the machine learning community.
Amateur machine learning enthusiasts are \textbf{unimportant users}.

\subsection{User Participation}

Thor Jonsson and Dr.\ Taylor will be expected to assist in supporting our group
with their domain knowledge of deep learning methods. They will also be
expected to participate in shaping the interfaces to the product (both the web
interface and the programming interface to the database) by using the
prototypes of those interfaces and providing feedback.

The minimum amount of participation from Dr.\ Taylor and Thor would be
participation in a meeting with our group members on a bi-weekly to monthly
basis, as well as participating in weekly correspondence electronically (e.g.
by e-mail).

\subsection{Maintenance Users and Service Technicians}

Maintenance users would certainly be members of Dr.\ Taylor's research group,
as they will be using the software produced by the project after its completion
and may need to add changes to the product.

Once the product is open-sourced into the community, maintenance users could
range from machine learning researchers to amateur machine learning
enthusiasts. These users could be expected to fix bugs or add new features that
were not in the initial scope of the project.

\chapter{Project Constraints}

Note that constraints also use the requirement shell, and therefore also use
the requirement numbering.

Customer satisfaction and dissatisfaction have been evaluated in the "Priority" entry.

Also there are no conflicts between requirements, or supporting materials, so
these entries have been dropped.

\section{Mandated Constraints}

\subsection{Solution Constraints}

\requirement
{Constraint_DeepLearningMethods}
{4a. Solution Constraint}
{software-skeleton-event}
{The human pose estimation component should use deep learning methods.}
{This constraint is to allow Dr.\ Taylor's group to integrate the software into
 their existing text-to-motion pipeline.}
{Dr.\ Graham Taylor}
{Dr.\ Taylor should confirm that the deep learning methods used in the human
 pose estimator are satisfactory.}
{High}
{Created September 26th, 2016.}

\requirement
{Constraint_DataStorageFormat}
{4a. Solution Constraint}
{database-text-to-motion-event}
{Use a standard format such as LMDB or HDF5 for storing text-motion data.}
{Having the data in a standard format will enable users to re-use existing code
 to manipulate that data.}
{Thor Jonsson }
{Run a set of existing tests to manipulate the standard data format (e.g. LMDB)
 and assert that those tests must pass.}
{High}
{Created October 3rd, 2016.}

\subsection{Implementation Environment of the Current System}

\requirement
{Constraint_Linux}
{4b. Implementation Environment}
{software-skeleton-event}
{The Text-to-Motion Software Suite must run under Linux.}
{Linux is the operating system used by the Guelph Machine Learning research
 lab, and also the most commonly used operating system in the research
 community.}
{Dr.\ Graham Taylor}
{Automated builds and testing should pass on popular Linux distributions:
 Ubuntu, Fedora and RHEL.}
{High}
{Created September 26th, 2016.}

\requirement
{Constraint_PythonAPI}
{4b. Implementation Environment}
{software-skeleton-event}
{Major APIs to the Text-to-Motion database must be accessible from the Python
 programming language.}
{Python is the language used by the rest of Dr.\ Taylor's text-to-motion
 pipeline. Python is a popular, easy-to-use, and quick-to-prototype language,
 and is therefore one of the most favoured programming languages among the
 Machine Learning research community.}
{Dr.\ Graham Taylor}
{There must be hooks to all major interfaces written in Python, and there must
 be tests that are directly testing the Python interfaces.}
{High}
{Created September 26th, 2016.}

\subsection{Partner or Collaborative Applications}

The McMaster Text-to-Motion database software will collaborate with the rest of
the University of Guelph group's text-to-motion pipeline. While their
application is still in a research stage, we can expect that their application
will be written in Python.

Furthermore, the animation component of Dr.\ Taylor's group's project will be
using the proprietary software application Muvizu.

\subsection{Off-the-Shelf Software}

There are a number of off-the-shelf libraries that can and should be used by
this project in order to implement the requirements. In particular, deep
learning libraries should be used for the training of neural networks and for
doing numerical computations.

Two notable deep learning frameworks are listed below.

\textbf{Caffe}, originally developed by the Berkeley Vision and Learning Center
(BVLC),  is a deep learning framework written in C++ with Python and Matlab
wrappers.

Caffe has a strong ConvNet implementation and is popular amongst the
computer vision community. However, Caffe's RNN implementation and support for
language models is lacking compared with other libraries.

Caffe is released under the BSD 2-Clause license.

\textbf{TensorFlow}, originally developed by researchers at Google, is a deep
learning and numerical computation framework written in C++ and Python.

TensorFlow has a clean, modular architecture and since it provides both Python
and C++ interfaces, code can be prototyped in a rich high-level interpreted
language before deployment in a high-performance, scalable environment.

TensorFlow is open sourced under the Apache 2.0 open source license.

It will also be necessary to use a library for accessing video codecs. This is
in order to convert video streams to and from sets of images, which will be
input to the human pose estimation software.

\textbf{FFmpeg} is a leading multimedia framework that is able to decode and
encode video, in addition to having many other features. FFmpeg can be utilized
for any video encoding and decoding purposes in the project.

To implement search-by-text functionality into the McMaster Text-to-Motion
Database, it will be helpful to leverage an existing software package that
implements full-text search.

\textbf{Sphinx} is such a software package, as it implements full-text search.
Sphinx also integrates well with SQL databases.

Sphinx is licensed under GPL version 2.

\subsection{Anticipated Workplace Environment}

As the McMaster Text-to-Motion Database is a software product, there are no
requirements considerations specifically pertaining to the anticipated
workplace environment of users of the product. Any considerations of
environment, such as operating system environment, have already been covered
under the ``Implementation Environment'' section.

\subsection{Schedule Constraints}

\requirement
{Constraint_DemoSchedule}
{4f. Schedule Constraint }
{software-skeleton-event}
{A demonstration of the product must be completed by February 13th, 2017.}
{This is a deadline that is part of the CS 4ZP6 course.}
{Dr.\ Wen Bo He}
{By February 13th, 2017, fully functional versions of the web interface,
 database interface and human pose estimation software must all be working as
 verified by Dr.\ He.}
{High}
{Created September 26th, 2016.}

\requirement
{Constraint_ProjectDeadline}
{4f. Schedule Constraint}
{software-skeleton-event}
{The project must be completed by April 5th, 2017.}
{The project is part of the CS 4ZP6 Capstone Project course.}
{Dr.\ Wen Bo He}
{All documentation, testing and implementation must be completed and checked in
 to GitHub by April 5th, 2017.}
{High}
{Created September 21th, 2016.}

\subsection{Budget Constraints}

There is no budget allocated for this project. As such, hardware requirements
such as GPUs to carry out the computations required for the project must be
provided by the client, Dr.\ Taylor, or borrowed through other means such as
seeking help from McMaster professors.

\section{Naming Conventions and Definitions}

\subsection{Definitions of All Terms, Including Acronyms, Used in the Project}

\textbf{The Project} when used, is referring to the McMaster Text to Motion
Database project. The project aims to generate a database of human pose
estimation model information that is linked to videos of human motion
containing rich text annotations.

\textbf{Human Pose Estimation} is the process of estimating the configuration,
or pose, of the body based on a single still image or a sequence of images that
comprise a video. Human pose estimation may find the chin, radius, humerus, and
other bone and joint positions.

\textbf{Charades} is a dataset composed of approximately 10K videos of daily
indoor activities, complete with associated action-describing sentences,
collected through Amazon Mechanical Turk\cite{charades}.

\textbf{MSR-VTT}, standing for ``Microsoft Research Video to Text'', is a
large-scale video benchmark for the task of translating video to text. MSR-VTT
provides 10K video clips spanning 41.2 hours and containing 200K clip-sentence
pairs in total\cite{msr-vtt}.

\textbf{Feedforward Neural Networks} are artifical neural networks where
connections between the units do \textit{not} form a cycle). They are the
simplest type of neural network, because information moves in only one
direction.

\textbf{ConvNets} or \textbf{Convolutional Neural Networks} are a type of
feed-forward artificial neural network. ConvNets are inspired by the visual
cortex and are commonly used in visual recognition applications.

\textbf{RNNs} or \textbf{Recurrent Neural Networks} are a class of artificial
neural networks where units form a directed cycle, in contrast with
feed-forward neural networks.

\textbf{Deep Belief Networks} are a type of deep neural network composed of
multiple layers of ``hidden units'' (variables that are not observable), with
connections between layers but not between units of a given layer.

\textbf{Multi-modal neural language models} are models of natural language that
can be conditioned on other modalities, e.g. high-level image
features\cite{DBLP:journals/corr/KirosSZ14}.

\textbf{Muvizu} is an interactive 3D animation package designed for quick storytelling.

An \textbf{autoencoder} is an artificial neural network used to learn a
representation of a set of data. The simplest form of an autoencoder is a
feed-forward neural net with an input layer, an output layer, and one or more
hidden layers. Instead of being trained to predict an output $Y$ from input
$X$, an autoencoder is trained to reconstruct its own input $X'$ from $X$.

\subsection{Data Dictionary for any Included Models}

Here we define the different information flows displayed in Figure
\ref{work-context-diagram}, the work context diagram.

A \textbf{skeleton overlay} refers to a set of human joint and body-part
estimation data that has been associated with a given video stream. A skeleton
overlay may also encompass an actual graphical stick-figure, displayed on top
of the video stream, with circles indicating the position of each joint.

\textbf{Text annotated video} refers to a video that has a time-stamped
natural-language description associated with it. Similarly, \textbf{text
annotated motion data} refers to human pose data that is time-stamped and
coupled with a natural-language description of the actions comprised by those
human pose data.

\section{Relevant Facts and Assumptions}

\subsection{Facts}

\begin{itemize}
        \item The training of deep neural networks can take on the order of
                days to weeks. For example, training two of the different
                neural network architectures used in
                \cite{DBLP:journals/corr/PfisterCZ15} on four NVIDIA GTX Titan
                GPUs took 3 and 7 days, respectively.
        \item ConvNets take a fixed size input and generate a fixed size
                output, whereas RNNs can handle arbitrary input/output lengths.
                This is one reason why ConvNets are more popular in vision
                applications, while RNNs are used for language applications.
                However, neural network architectures can be used that combine
                both ConvNets and RNNs.
        \item The uncompressed size of the Charades dataset is 55GB at full
                resolution, and 16GB at 480p.
        \item The goals of this project, specifically generating a database of
                text-annotated human motion data, constitute new research and
                there are no existing solutions available.
\end{itemize}

\subsection{Assumptions}

\begin{itemize}
        \item It has been assumed that it is possible to re-use existing
                software components in order to achieve the human pose
                estimation use-case, as it would take a full eight months just
                to implement an algorithm such as that of
                \cite{DBLP:journals/corr/PfisterCZ15} from scratch.
        \item It has been assumed that this project will not encompass
                generating a dataset of videos with text annotations, and that
                we will be able to rely on an existing dataset such as the
                Charades dataset.
        \item It is assumed that a storage format for human pose estimation
                data already exists, and will be available for this project to
                leverage.
\end{itemize}

\chapter{Functional Requirements}

\section{The Scope of the Work}

\subsection{The Current Situation}

There is a large amount of existing research into human pose estimation, which
this project will leverage. Based on Constraint
\ref{Constraint_DeepLearningMethods}, we focus on existing solutions that use
deep learning methods.

\cite{DBLP:journals/corr/PfisterCZ15} present a ConvNet architecture for human
pose estimation from videos, which is able to benefit from temporal context
across multiple frames using optical flow. This work is focused on upper-body
human pose estimation only.

\cite{DBLP:journals/corr/BelagiannisZ16} propose a ConvNet model for predicting
2D human body poses in an image. This model is able to achieve state-of-the-art
results using a simple architecture, and draws on the work done in
\cite{DBLP:journals/corr/PfisterCZ15}.

\cite{DBLP:journals/corr/WeiRKS16} introduces \textit{Convolutional Pose
Machines (CPMs)} for pose estimation in images. CPMs consist of a sequence of
ConvNets that iteratively produce 2D belief maps. Source code from this paper
is available, and makes use of the Caffe deep learning library.

In \cite{zhou2016sparseness}, the authors estimate 3D full-body human poses
from a monocular image sequence. They train a ConvNet to predict the locations
of joints in 2D, then estimate human poses in 3D via an
Expectation-Maximization algorithm over the entire video sequence.

On the animation side of the larger computational storytelling problem our
project is assisting in solving, \cite{Holden2016} use
deep learning to synthesize character movements based on high-level parameters
such as a curve that the character should follow. The learned motion is
represented by the hidden units of a convolutional autoencoder.

\subsection{The Context of the Work}

The context diagram, displaying adjacent systems to this project, is shown in
Figure \ref{work-context-diagram}.

\begin{figure}
        \caption{McMaster Text-to-Motion Database Work Context Diagram}
        \label{work-context-diagram}
        \centering
        \includegraphics[width=0.8\textwidth]{mcmaster-text-to-motion-work-context-diagram.png}
\end{figure}

\subsection{Work Partitioning}

The events/use-cases for the project are laid out in Table \ref{business-event-list-table}.

\begin{table}
\begin{enumerate}
\caption{Business Event List}
\label{business-event-list-table}
    \begin{tabular}{  p{1cm} | p{3cm} | p{5cm} | p{5cm} }
    \hline
    Event \# & Event Name & Input and Output & Summary \\
    \hline
    \item \label{web-skeleton-event}
            & Web Interface Skeleton Overlay
            & \textbf{IN}: An image or video with humans in it.\newline
            \textbf{OUT}: The same image or video, with a skeleton overlaid on
            top of all humans indicating their bone and joint positions.
            & Allow users to observe the human pose estimation component in
            real time through a web interface.\\
    \item \label{web-text-to-motion-event}
            & Web Interface Text-to-Motion
            & \textbf{IN}: Word or phrase describing a human pose or action.\newline
            \textbf{OUT}: Rich-text-annotated video corresponding to the input
            word/phrase, complete with overlaid skeleton.
            & Allow users to see the output of searches on the database using
            pose and/or action keywords, such as ``run'' or ``kneeling''.\\
    \item \label{software-skeleton-event}
            & Software Interface Skeleton Overlay
            & \textbf{IN}: A stream of video with humans depicted.\newline
            \textbf{OUT}: A set of human pose estimations corresponding to the
            video, in a standard data format.
            & Users should be able to use the human pose estimation solution to
            generate their own motion data set.\\
    \item \label{database-text-to-motion-event}
            & Database Interface Text-to-Motion
            & \textbf{IN}: Word or phrase describing a human pose or action.\newline
            \textbf{OUT}: Video in common encoding (e.g. MP4), associated
            rich-text-annotations, and human pose estimations in a standardized
            format.
            & Provide users direct access to the raw motion-estimation data
            format based on action-keyword database lookup.\\
    \hline
    \end{tabular}
\end{enumerate}
\end{table}

\section{The Scope of the Product}

\subsection{Product Boundary}

We do not include a product boundary diagram, and instead refer to Figure
\ref{work-context-diagram}.

\subsection{Product Use-case List}

\subsection{Individual Product Use Cases}

\section{Functional and Data Requirements}

\subsection{Functional Requirements}

\requirement
{Req_FrameEncodeDecode}
{9a. Functional Requirement}
{software-skeleton-event}
{The text-to-motion software suite will provide an API to read individual
 frames in RGB format from a video stream. At least MP4, MP2 and AAC must be
 supported.}
{Researchers may wish to do their own processing on RGB frames before feeding
 those frames into the human pose estimation module.}
{For a given set of test video streams, the frame-capture API must produce RGB
 frames identical to known reference frames.}
{Moderate}
{Created October 5th, 2016.}
{Brendan Duke}

\subsection{Data Requirements}

\chapter{Nonfunctional Requirements}

\section{Look and Feel Requirements}

\subsection{Appearance Requirements}

\subsection{Style Requirements}

\section{Usability and Humanity Requirements}

\subsection{Ease of Use Requirements}

\subsection{Personalization and Internationalization Requirements}

\subsection{Learning Requirements}

\subsection{Understandability and Politeness Requirements}

\subsection{Accessibility Requirements}

\section{Performance Requirements}

\subsection{Speed and Latency Requirements}

\subsection{Safety-Critical Requirements}

\subsection{Precision or Accuracy Requirements}

\subsection{Reliability and Availability Requirements}

\subsection{Robustness or Fault-Tolerance Requirements}

\subsection{Capacity Requirements}

\subsection{Scaling of Extensibility Requirements}

\subsection{Longevity Requirements}

\section{Operational and Environmental Requirements}

\subsection{Expected Physical Environment}

\subsection{Requirements for Interfacing with Adjacent Systems}

\subsection{Productization Requirements}

\subsection{Release Requirements}

\section{Maintainability and Support Requirements}

\subsection{Maintenance Requirements}

\subsection{Supportability Requirements}

\subsection{Adaptability Requirements}

\section{Security Requirements}

\subsection{Access Requirements}

\subsection{Integrity Requirements}

\subsection{Privacy Requirements}

\subsection{Audit Requirements}

\subsection{Immunity Requirements}

\section{Cultural and Political Requirements}

\subsection{Cultural Requirements}

\subsection{Political Requirements}

\section{Legal Requirements}

\subsection{Compliance Requirements}

\subsection{Standards Requirements}

\chapter{Project Issues}

\section{Open Issues}

\section{Off-the-Shelf Solutions}

\subsection{Ready-Made Products}

\subsection{Reusable Components}

\subsection{Products That Can Be Copied}

\section{New Problems}

\subsection{Effects on the Current Environment}

\subsection{Effects on the Installed Systems}

\subsection{Potential User Problems}

\subsection{Limitations in the Anticipated Implementation Environment That May
            Inhibit the New Product}

\subsection{Follow-Up Problems}

\section{Tasks}

\subsection{Project Planning}

\subsection{Planning of the Development Phases}

\section{Migration to the New Product}

\subsection{Requirements for Migration of the New Product}

\subsection{Data That Has to Be Modified or Translated for the New
            System}

\section{Risks}

\section{Costs}

\section{User Documentation and Training}

\subsection{User Documentation Requirements}

\subsection{Training Requirements}

\section{Waiting Room}

\section{Ideas for Solutions}

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,SoftwareRequirementsSpecification} 

\end{document}
